name: Atualização Semanal dos Dados de Dengue

on:
  schedule:
    - cron: '0 6 * * 1' 
  workflow_dispatch:

jobs:
  update-data-pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      id-token: write

    steps:
      - name: Checkout do repositório
        uses: actions/checkout@v4

      - name: Setup do Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Instalação das dependências
<<<<<<< HEAD
        working-directory: ./ai_predict/data/pipeline_scripts
=======
        # Define o diretório de trabalho para que o pip encontre o arquivo
        working-directory: ./ai_predict/data/montarDados
>>>>>>> 23618b9a54530acfa126731692d39cba85fc5722
        run: |
          python -m pip install --upgrade pip
          # Instala as dependências a partir do arquivo 'requirements.txt'
          pip install -r requirements.txt
          pip install huggingface_hub  # Instala a biblioteca do Hugging Face
<<<<<<< HEAD
=======

>>>>>>> 23618b9a54530acfa126731692d39cba85fc5722
      - name: Executa o pipeline de atualização de dados
        working-directory: ./ai_predict/data/montarDados
        run: python master_update.py

      - name: Upload dos dados de inferência para o Hugging Face Hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
<<<<<<< HEAD
        run: |
          python -c "import os; from huggingface_hub import HfApi; api = HfApi(token=os.environ['HF_TOKEN']); api.upload_file(path_or_fileobj='./ai_predict/data/inference_data.parquet', path_in_repo='inference_data.parquet', repo_id='previdengue/predict_inference_data', repo_type='dataset', commit_message='chore(data): Atualização semanal automática dos dados')"
=======
        run: |
          # Use a biblioteca huggingface_hub para um upload mais seguro e moderno.
          # Adicionamos repo_type='dataset' para especificar que o repositório é um dataset.
          python -c "import os; from huggingface_hub import HfApi; api = HfApi(token=os.environ['HF_TOKEN']); api.upload_file(path_or_fileobj='./ai_predict/data/inference_data.parquet', path_in_repo='inference_data.parquet', repo_id='previdengue/predict_inference_data', repo_type='dataset', commit_message='chore(data): Atualização semanal automática dos dados')"

      - name: Commit dos novos ficheiros de dados
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add -A
          git commit -m "chore(data): Atualização semanal automática dos dados" || echo "Nenhuma alteração nos dados para fazer commit."
          git push

>>>>>>> 23618b9a54530acfa126731692d39cba85fc5722
