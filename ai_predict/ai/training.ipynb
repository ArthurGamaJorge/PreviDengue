{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializando\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-forecasting lightning numpy pandas pyarrow matplotlib tqdm_joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "# --- Configuração de Paths e Constantes ---\n",
    "BASE_DIR = \"/content/drive/MyDrive/lstm_projeto\"\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"final_training_data.parquet\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"model_checkpoint.keras\")\n",
    "EPOCH_TRACK_PATH = CHECKPOINT_PATH.replace(\".keras\", \"_epoch.txt\")\n",
    "\n",
    "SCALER_DIR = os.path.join(BASE_DIR, \"scalers\")\n",
    "\n",
    "# Cria os diretórios no Google Drive se eles não existirem\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SCALER_DIR, exist_ok=True) \n",
    "\n",
    "print(\"Paths configurados com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences_multi_step(data: np.ndarray, seq_len: int, horizon: int):\n",
    "    num_samples = data.shape[0] - seq_len - horizon + 1\n",
    "    if num_samples < 0: return np.array([]), np.array([])\n",
    "    \n",
    "    X = np.zeros((num_samples, seq_len, data.shape[1]), dtype=np.float32)\n",
    "    y = np.zeros((num_samples, horizon), dtype=np.float32)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        X[i] = data[i : i + seq_len]\n",
    "        y[i] = data[i + seq_len : i + seq_len + horizon, 0] # O alvo é a coluna 'numero_casos'\n",
    "    return X, y\n",
    "    \n",
    "    \n",
    "class PersistentEarlyStoppingCityVal(Callback):\n",
    "    def __init__(self, city_val_callback, patience=5, restore_best_weights=True, state_file=\"earlystop_city_state.json\"):\n",
    "        super().__init__()\n",
    "        self.city_val_callback = city_val_callback\n",
    "        self.patience = patience\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.state_file = state_file\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        self.best = np.inf\n",
    "\n",
    "        if os.path.exists(self.state_file):\n",
    "            with open(self.state_file, \"r\") as f:\n",
    "                try:\n",
    "                    state = json.load(f)\n",
    "                    self.wait = int(state.get(\"wait\", 0))\n",
    "                    self.best = float(state.get(\"best\", np.inf))\n",
    "                except Exception:\n",
    "                    self.wait = 0\n",
    "                    self.best = np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"city_val_loss\")\n",
    "        if current is None:\n",
    "            self.city_val_callback.on_epoch_end(epoch, logs)\n",
    "            current = logs.get(\"city_val_loss\")\n",
    "            if current is None:\n",
    "                return\n",
    "\n",
    "        if current < self.best:\n",
    "            self.best = float(current)\n",
    "            self.wait = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "\n",
    "        with open(self.state_file, \"w\") as f:\n",
    "            json.dump({\"wait\": int(self.wait), \"best\": float(self.best)}, f)\n",
    "\n",
    "        if self.wait >= self.patience:\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                self.model.set_weights(self.best_weights)\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch + 1} (city_val_loss: {self.best:.4f})\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "class EpochSaver(Callback):\n",
    "    \"\"\"\n",
    "    Callback simples para salvar o número da última época concluída.\n",
    "    \"\"\"\n",
    "    def __init__(self, epoch_file_path):\n",
    "        super().__init__()\n",
    "        self.epoch_file_path = epoch_file_path\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with open(self.epoch_file_path, \"w\") as f:\n",
    "            f.write(str(epoch))\n",
    "\n",
    "\n",
    "def process_municipio(municipio_id):\n",
    "    \"\"\"\n",
    "    Processa os dados de um único município: cria sequências, divide em treino/teste\n",
    "    e aplica os scalers globais pré-ajustados.\n",
    "    \"\"\"\n",
    "    df_mun = df[df[\"codigo_ibge\"] == municipio_id].copy()\n",
    "    if len(df_mun) < SEQUENCE_LENGTH + HORIZON:\n",
    "        return None\n",
    "\n",
    "    dynamic_data = df_mun[dynamic_features].values\n",
    "    static_data = df_mun[static_features].iloc[0].values.reshape(1, -1)\n",
    "\n",
    "    X_mun_raw, y_mun_raw = create_sequences_multi_step(dynamic_data, SEQUENCE_LENGTH, HORIZON)\n",
    "    if X_mun_raw.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    static_seq = np.repeat(static_data, len(X_mun_raw), axis=0)\n",
    "\n",
    "    split_idx = int(len(X_mun_raw) * 0.8)\n",
    "    X_train_raw, y_train_raw, static_train = (\n",
    "        X_mun_raw[:split_idx], y_mun_raw[:split_idx], static_seq[:split_idx]\n",
    "    )\n",
    "    X_test_raw, y_test_raw, static_test = (\n",
    "        X_mun_raw[split_idx:], y_mun_raw[split_idx:], static_seq[split_idx:]\n",
    "    )\n",
    "\n",
    "    if X_train_raw.shape[0] == 0 or X_test_raw.shape[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    # --- CORREÇÃO AQUI: Carrega os scalers do diretório único 'SCALER_DIR' ---\n",
    "    scaler_dyn = joblib.load(os.path.join(SCALER_DIR, \"scaler_dyn_global.pkl\"))\n",
    "    scaler_static = joblib.load(os.path.join(SCALER_DIR, \"scaler_static_global.pkl\"))\n",
    "    scaler_target = joblib.load(os.path.join(SCALER_DIR, \"scaler_target_global.pkl\"))\n",
    "    \n",
    "    # Aplica a transformação (.transform) nos dados\n",
    "    X_train_2d = X_train_raw.reshape(-1, X_train_raw.shape[-1])\n",
    "    X_test_2d = X_test_raw.reshape(-1, X_test_raw.shape[-1])\n",
    "\n",
    "    X_train_scaled = scaler_dyn.transform(X_train_2d).reshape(X_train_raw.shape)\n",
    "    X_test_scaled = scaler_dyn.transform(X_test_2d).reshape(X_test_raw.shape)\n",
    "    static_train_scaled = scaler_static.transform(static_train)\n",
    "    static_test_scaled = scaler_static.transform(static_test)\n",
    "    y_train_scaled = scaler_target.transform(y_train_raw)\n",
    "    y_test_scaled = scaler_target.transform(y_test_raw)\n",
    "\n",
    "    # Retorna também o y_train_raw para calcular o pico de treino para a métrica\n",
    "    return (\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        X_test_scaled, y_test_scaled,\n",
    "        static_train_scaled, static_test_scaled,\n",
    "        y_train_raw\n",
    "    )\n",
    "\n",
    "\n",
    "class CityValLossWeighted(Callback):\n",
    "    def __init__(self, X_val, static_val, y_val, city_sizes, city_weights, save_path):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.static_val = static_val\n",
    "        self.y_val = y_val\n",
    "        self.city_sizes = city_sizes\n",
    "        self.city_weights = city_weights # Pesos pré-calculados\n",
    "        self.save_path = save_path\n",
    "        self.best = np.inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict([self.X_val, self.static_val], batch_size=1024, verbose=0)\n",
    "        start = 0\n",
    "        weighted_losses = []\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for i, n in enumerate(self.city_sizes):\n",
    "            y_true_city = self.y_val[start:start+n]\n",
    "            y_pred_city = y_pred[start:start+n]\n",
    "            start += n\n",
    "\n",
    "            mse_city = np.mean((y_true_city - y_pred_city)**2)\n",
    "\n",
    "            # Usa o peso pré-calculado do conjunto de treino\n",
    "            weight = self.city_weights[i]\n",
    "\n",
    "            weighted_losses.append(mse_city * weight)\n",
    "            total_weight += weight\n",
    "\n",
    "        city_val_loss = np.sum(weighted_losses) / max(total_weight, 1e-8)\n",
    "        logs[\"city_val_loss\"] = city_val_loss\n",
    "\n",
    "        if city_val_loss < self.best:\n",
    "            self.best = city_val_loss\n",
    "            self.model.save(self.save_path)\n",
    "            print(f\"\\nEpoch {epoch+1}: Melhor modelo salvo com city_val_loss ponderado: {city_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMN = \"numero_casos\"\n",
    "SEQUENCE_LENGTH = 12\n",
    "HORIZON = 8\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- 4. CARREGAMENTO E PRÉ-PROCESSAMENTO DOS DADOS ---\n",
    "print(\"\\n--- Etapa 4: Carregando e processando os dados ---\")\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "df = df.sort_values(by=[\"codigo_ibge\", \"ano\", \"semana\"])\n",
    "\n",
    "df[\"week_sin\"] = np.sin(2 * np.pi * df[\"semana\"] / 52)\n",
    "df[\"week_cos\"] = np.cos(2 * np.pi * df[\"semana\"] / 52)\n",
    "df[\"year_norm\"] = (df[\"ano\"] - df[\"ano\"].min()) / (df[\"ano\"].max() - df[\"ano\"].min())\n",
    "\n",
    "dynamic_features = [\n",
    "    \"numero_casos\", \"T2M\", \"T2M_MAX\", \"T2M_MIN\",\n",
    "    \"PRECTOTCORR\", \"RH2M\", \"ALLSKY_SFC_SW_DWN\",\n",
    "    \"week_sin\", \"week_cos\", \"year_norm\"\n",
    "]\n",
    "static_features = [\"latitude\", \"longitude\"]\n",
    "municipios = df[\"codigo_ibge\"].unique()\n",
    "\n",
    "# --- Etapa 4.5: Preparando ou Carregando Scalers Globais ---\n",
    "\n",
    "# Define o caminho para um dos scalers para verificar sua existência\n",
    "scaler_dyn_path = os.path.join(SCALER_DIR, \"scaler_dyn_global.pkl\")\n",
    "\n",
    "# <> CORREÇÃO: Verifica se os scalers já existem antes de criá-los\n",
    "if os.path.exists(scaler_dyn_path):\n",
    "    print(\"Scalers globais já existem. Pulando a etapa de criação.\")\n",
    "else:\n",
    "    print(\"Scalers globais não encontrados. Iniciando processo de criação (pode demorar)...\")\n",
    "    all_dyn_train, all_static_train, all_target_train = [], [], []\n",
    "    for mid in tqdm(municipios, desc=\"Coletando dados de treino para scalers\"):\n",
    "        df_mun = df[df[\"codigo_ibge\"] == mid]\n",
    "        if len(df_mun) < SEQUENCE_LENGTH + HORIZON: continue\n",
    "        \n",
    "        dyn_data = df_mun[dynamic_features].values\n",
    "        static_data = df_mun[static_features].iloc[0].values.reshape(1, -1)\n",
    "        X_raw, y_raw = create_sequences_multi_step(dyn_data, SEQUENCE_LENGTH, HORIZON)\n",
    "        if X_raw.shape[0] == 0: continue\n",
    "        \n",
    "        split_idx = int(len(X_raw) * 0.8)\n",
    "        if split_idx == 0: continue\n",
    "\n",
    "        all_dyn_train.append(X_raw[:split_idx].reshape(-1, X_raw.shape[-1]))\n",
    "        all_static_train.append(np.repeat(static_data, split_idx, axis=0))\n",
    "        all_target_train.append(y_raw[:split_idx])\n",
    "\n",
    "    scaler_dyn_global = MinMaxScaler().fit(np.concatenate(all_dyn_train))\n",
    "    scaler_static_global = MinMaxScaler().fit(np.concatenate(all_static_train))\n",
    "    scaler_target_global = MinMaxScaler().fit(np.concatenate(all_target_train))\n",
    "\n",
    "    # Salva os scalers no diretório único\n",
    "    joblib.dump(scaler_dyn_global, os.path.join(SCALER_DIR, \"scaler_dyn_global.pkl\"))\n",
    "    joblib.dump(scaler_static_global, os.path.join(SCALER_DIR, \"scaler_static_global.pkl\"))\n",
    "    joblib.dump(scaler_target_global, os.path.join(SCALER_DIR, \"scaler_target_global.pkl\"))\n",
    "    print(\"Scalers globais criados e salvos.\")\n",
    "\n",
    "# --- Processamento por município ---\n",
    "print(\"Iniciando processamento por município com scalers globais\")\n",
    "with tqdm_joblib(tqdm(desc=\"Processando municípios\", total=len(municipios))):\n",
    "    results = Parallel(n_jobs=6, backend='threading')(\n",
    "        delayed(process_municipio)(mid) for mid in municipios\n",
    "    )\n",
    "\n",
    "results = [r for r in results if r is not None]\n",
    "X_train_list, y_train_list, X_test_list, y_test_list, static_train_list, static_test_list, y_train_raw_list = zip(*results)\n",
    "print(\"Processamento por município concluído.\")\n",
    "\n",
    "# --- 5. AGREGAÇÃO FINAL DOS DADOS ---\n",
    "print(\"\\n--- Etapa 5: Agregando dados para o treinamento ---\")\n",
    "X_train = np.concatenate(X_train_list, axis=0)\n",
    "y_train = np.concatenate(y_train_list, axis=0)\n",
    "X_test = np.concatenate(X_test_list, axis=0)\n",
    "y_test = np.concatenate(y_test_list, axis=0)\n",
    "static_train = np.concatenate(static_train_list, axis=0)\n",
    "static_test = np.concatenate(static_test_list, axis=0)\n",
    "print(f\"Total de sequências de Treino: {X_train.shape[0]}, Teste: {X_test.shape[0]}\")\n",
    "\n",
    "city_sizes = [x.shape[0] for x in X_test_list]\n",
    "city_train_peaks = [np.max(y_raw) if y_raw.size > 0 else 1.0 for y_raw in y_train_raw_list]\n",
    "\n",
    "# --- 6. CRIAÇÃO OU CARREGAMENTO DO MODELO ---\n",
    "print(\"\\n--- Etapa 6: Carregando ou criando o modelo ---\")\n",
    "initial_epoch = 0\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Carregando modelo existente de: {CHECKPOINT_PATH}\")\n",
    "    model = load_model(CHECKPOINT_PATH)\n",
    "    if os.path.exists(EPOCH_TRACK_PATH):\n",
    "        with open(EPOCH_TRACK_PATH, \"r\") as f:\n",
    "            initial_epoch = int(f.read().strip()) + 1\n",
    "    print(f\"Continuando o treinamento a partir da época: {initial_epoch}\")\n",
    "else:\n",
    "    print(\"Nenhum checkpoint encontrado. Criando um novo modelo.\")\n",
    "    input_dyn = Input(shape=(SEQUENCE_LENGTH, len(dynamic_features)))\n",
    "    lstm_out = LSTM(50, return_sequences=True)(input_dyn)\n",
    "    lstm_out = LSTM(50)(lstm_out)\n",
    "    input_static = Input(shape=(len(static_features),))\n",
    "    concat = Concatenate()([lstm_out, input_static])\n",
    "    output = Dense(HORIZON)(concat)\n",
    "    model = Model(inputs=[input_dyn, input_static], outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    print(\"Novo modelo criado e compilado.\")\n",
    "\n",
    "# --- 7. TREINAMENTO DO MODELO ---\n",
    "print(\"\\n--- Etapa 7: Iniciando o treinamento do modelo ---\")\n",
    "epoch_saver = EpochSaver(EPOCH_TRACK_PATH)\n",
    "checkpoint_last = ModelCheckpoint(filepath=CHECKPOINT_PATH, save_best_only=False, save_freq=\"epoch\", verbose=1)\n",
    "\n",
    "city_val_loss_cb = CityValLossWeighted(\n",
    "    X_val=X_test,\n",
    "    static_val=static_test,\n",
    "    y_val=y_test,\n",
    "    city_sizes=city_sizes,\n",
    "    city_weights=city_train_peaks,\n",
    "    save_path=CHECKPOINT_PATH.replace(\".keras\", \"_best_city.keras\")\n",
    ")\n",
    "\n",
    "early_stopping_city = PersistentEarlyStoppingCityVal(\n",
    "    city_val_callback=city_val_loss_cb,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    state_file=os.path.join(BASE_DIR, \"earlystop_city_state.json\")\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, static_train], y_train,\n",
    "    validation_data=([X_test, static_test], y_test),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping_city, epoch_saver, city_val_loss_cb, checkpoint_last], \n",
    "    initial_epoch=initial_epoch,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"\\n--- TREINAMENTO FINALIZADO! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# --- 1. CONFIGURAÇÃO ---\n",
    "# Paths para os artefatos salvos\n",
    "BASE_DIR = \"/content/drive/MyDrive/lstm_projeto\"\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"checkpoints\", \"model_checkpoint_best_city.keras\")\n",
    "SCALER_DIR = os.path.join(BASE_DIR, \"scalers\")\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"inference_data.parquet\") \n",
    "\n",
    "# Constantes do modelo\n",
    "SEQUENCE_LENGTH = 12\n",
    "HORIZON = 6 # O modelo prevê 6 semanas\n",
    "\n",
    "# Features usadas no treino (a ordem DEVE ser a mesma)\n",
    "DYN_FEATS = [\n",
    "    \"numero_casos\", \"T2M\", \"T2M_MAX\", \"T2M_MIN\",\n",
    "    \"PRECTOTCORR\", \"RH2M\", \"ALLSKY_SFC_SW_DWN\",\n",
    "    \"week_sin\", \"week_cos\", \"year_norm\"\n",
    "]\n",
    "STATIC_FEATS = [\"latitude\", \"longitude\"]\n",
    "\n",
    "# Min/max do ano usado no TREINAMENTO para normalizar o ano\n",
    "YEAR_MIN_TRAIN = 2014\n",
    "YEAR_MAX_TRAIN = 2025 # Ajustado para o ano final real do seu treinamento.\n",
    "\n",
    "# --- 2. FUNÇÃO PRINCIPAL ---\n",
    "\n",
    "def main():\n",
    "    # --- 3. CARREGAMENTO DOS ARTEFATOS ---\n",
    "    print(\"Carregando modelo e scalers...\")\n",
    "    try:\n",
    "        model = load_model(MODEL_PATH)\n",
    "        scaler_dyn = joblib.load(os.path.join(SCALER_DIR, \"scaler_dyn_global.pkl\"))\n",
    "        scaler_static = joblib.load(os.path.join(SCALER_DIR, \"scaler_static_global.pkl\"))\n",
    "        scaler_target = joblib.load(os.path.join(SCALER_DIR, \"scaler_target_global.pkl\"))\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro: Arquivo não encontrado. Verifique os paths. Detalhe: {e}\")\n",
    "        return\n",
    "    print(\"Modelo e scalers carregados.\")\n",
    "\n",
    "    # --- 4. PREPARAÇÃO DOS DADOS ---\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    df['codigo_ibge'] = df['codigo_ibge'].astype(int)\n",
    "    df['ano'] = df['ano'].astype(int)\n",
    "    df['semana'] = df['semana'].astype(int)\n",
    "    df = df.sort_values(by=[\"codigo_ibge\", \"ano\", \"semana\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Adiciona uma coluna de data para facilitar manipulação\n",
    "    df['date'] = pd.to_datetime(df['ano'].astype(str) + df['semana'].astype(str) + '1', format='%Y%W%w')\n",
    "\n",
    "    # Seleção do município\n",
    "    ibge = int(input(\"Digite o código IBGE do município: \").strip())\n",
    "    df_mun = df[df['codigo_ibge'] == ibge].copy().reset_index(drop=True)\n",
    "    \n",
    "    if df_mun.empty:\n",
    "        print(f\"Município com IBGE {ibge} não encontrado nos dados.\")\n",
    "        return\n",
    "    \n",
    "    mun_name = df_mun['municipio'].iloc[0]\n",
    "    print(f\"Preparando previsão para: {mun_name} ({ibge})\")\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_mun[\"week_sin\"] = np.sin(2 * np.pi * df_mun[\"semana\"] / 52)\n",
    "    df_mun[\"week_cos\"] = np.cos(2 * np.pi * df_mun[\"semana\"] / 52)\n",
    "    df_mun[\"year_norm\"] = (df_mun[\"ano\"] - YEAR_MIN_TRAIN) / (YEAR_MAX_TRAIN - YEAR_MIN_TRAIN)\n",
    "\n",
    "    # --- 5. LÓGICA DE PREVISÃO CORRIGIDA ---\n",
    "    # <> CORREÇÃO: Permite ao usuário escolher o ponto de partida da previsão.\n",
    "    start_year = int(input(f\"Digite o ANO de início da previsão (ex: 2025): \").strip())\n",
    "    start_week = int(input(f\"Digite a SEMANA de início da previsão (ex: 24): \").strip())\n",
    "\n",
    "    # Encontra o índice da semana ANTERIOR à semana de início da previsão\n",
    "    prediction_point_idx = df_mun.index[\n",
    "        (df_mun['ano'] == start_year) & (df_mun['semana'] == start_week)\n",
    "    ].tolist()\n",
    "\n",
    "    if not prediction_point_idx:\n",
    "        print(f\"Erro: A semana {start_week} do ano {start_year} não foi encontrada nos dados.\")\n",
    "        return\n",
    "        \n",
    "    last_known_idx = prediction_point_idx[0] - 1\n",
    "\n",
    "    if last_known_idx < SEQUENCE_LENGTH - 1:\n",
    "        print(f\"Erro: Histórico insuficiente. São necessárias pelo menos {SEQUENCE_LENGTH} semanas de dados antes de {start_year}/{start_week}.\")\n",
    "        return\n",
    "\n",
    "    # A sequência de entrada são as 12 semanas que terminam no ponto escolhido.\n",
    "    start_idx = last_known_idx - SEQUENCE_LENGTH + 1\n",
    "    end_idx = last_known_idx\n",
    "    input_sequence_df = df_mun.iloc[start_idx : end_idx + 1]\n",
    "    \n",
    "    last_known_date = input_sequence_df['date'].iloc[-1]\n",
    "    print(f\"Usando dados até {last_known_date.strftime('%Y-%m-%d')} para prever a partir de {start_year}/{start_week}.\")\n",
    "\n",
    "    static_raw = input_sequence_df[STATIC_FEATS].iloc[0].values.reshape(1, -1)\n",
    "    dyn_hist_raw = input_sequence_df[DYN_FEATS].values\n",
    "    \n",
    "    input_dyn_scaled = scaler_dyn.transform(dyn_hist_raw).reshape(1, SEQUENCE_LENGTH, len(DYN_FEATS))\n",
    "    input_static_scaled = scaler_static.transform(static_raw)\n",
    "\n",
    "    # --- 6. EXECUÇÃO DA PREVISÃO ---\n",
    "    print(f\"Executando o modelo para prever as próximas {HORIZON} semanas...\")\n",
    "    prediction_scaled = model.predict([input_dyn_scaled, input_static_scaled])\n",
    "    \n",
    "    prediction_cases = scaler_target.inverse_transform(prediction_scaled).flatten()\n",
    "    \n",
    "    # --- 7. VISUALIZAÇÃO ---\n",
    "    print(\"Previsão concluída. Gerando gráfico...\")\n",
    "    \n",
    "    display_hist_start_idx = max(0, last_known_idx - 24)\n",
    "    hist_to_display = df_mun.iloc[display_hist_start_idx : last_known_idx + 1]\n",
    "    \n",
    "    future_start_idx = last_known_idx + 1\n",
    "    future_end_idx = last_known_idx + HORIZON\n",
    "    future_to_display = df_mun.iloc[future_start_idx : future_end_idx + 1]\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    last_known_case = hist_to_display['numero_casos'].iloc[-1]\n",
    "    connected_prediction = np.insert(prediction_cases, 0, last_known_case)\n",
    "    \n",
    "    x_hist = np.arange(len(hist_to_display))\n",
    "    x_future_real = np.arange(len(hist_to_display), len(hist_to_display) + len(future_to_display))\n",
    "    x_pred = np.arange(len(hist_to_display) - 1, len(hist_to_display) - 1 + len(connected_prediction))\n",
    "\n",
    "    plt.plot(x_hist, hist_to_display['numero_casos'], 'o-', label=\"Histórico de Casos Conhecido\")\n",
    "    plt.plot(x_future_real, future_to_display['numero_casos'], 'o-', color='green', label=\"Futuro (Real)\")\n",
    "    plt.plot(x_pred, connected_prediction, 'o--', color='red', label=f\"Previsão de Casos (Próximas {HORIZON} semanas)\")\n",
    "\n",
    "    all_dates_df = pd.concat([hist_to_display, future_to_display])\n",
    "    tick_labels = [f\"{row.ano}/{row.semana:02d}\" for index, row in all_dates_df.iterrows()]\n",
    "    tick_positions = np.arange(len(all_dates_df))\n",
    "    \n",
    "    plt.xticks(ticks=tick_positions, labels=tick_labels, rotation=45, ha=\"right\")\n",
    "    \n",
    "    if len(tick_positions) > 30:\n",
    "        plt.gca().xaxis.set_major_locator(plt.MaxNLocator(30))\n",
    "\n",
    "    plt.axvline(x=len(hist_to_display) - 1, color=\"black\", linestyle=\":\", linewidth=2, label=f\"Ponto da Previsão\")\n",
    "    plt.title(f\"Previsão de Casos de Dengue - {mun_name} ({ibge})\")\n",
    "    plt.xlabel(\"Ano/Semana Epidemiológica\")\n",
    "    plt.ylabel(\"Número de Casos Semanais\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
